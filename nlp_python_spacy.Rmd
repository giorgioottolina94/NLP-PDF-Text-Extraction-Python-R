---
title: "NLP, PDF Scraping & Sentiment Prediction"
output: 
   html_document:
       theme: flatly
       toc: true
       toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo    = TRUE,
    message = FALSE,
    warning = FALSE
)
```


```{r, echo=F}
knitr::include_graphics("img/shiny_app_code.jpg")
```


```
conda install spacy -n py3.6
```

# Libraries

```{r}
# PDF
library(pdftools)
# library(tesseract) # If OCR is needed

# Images
library(magick)
library(knitr)

# Core
library(tidyverse)
library(plotly)
library(tidyquant)
```

```{r}
library(reticulate)

# Replace this with your conda environment 
use_condaenv("py3.6", required = TRUE)
```

# R (DATA PREP)

PDF Tools Package - https://docs.ropensci.org/pdftools/

Haliburton's 10Q Filing for Q1 2020: https://ir.halliburton.com/financial-information/quarterly-results

```{r}
pdf_path <- "pdf/Halliburton Announces First Quarter 2020 Results.pdf"
```

## PDF Meta Data

About the PDF. 

```{r}
pdf_info(pdf_path)
```


```{r}
pdf_length(pdf_path)
```

## Working with Images

### Magick

_Resource:_ https://docs.ropensci.org/magick/

#### Read PDF

```{r}
img_page_1 <- image_read_pdf(pdf_path, pages = 1)

img_page_1 %>% image_scale("600")
```

#### Write to PNG

```{r}
img_page_1 %>% 
    image_scale("600") %>%
    image_write(path = "img/page1.png", format = "png")
```


#### Read PNG

```{r}
image_read("img/page1.png")
```



## PDF Text Data

### Extract data

```{r}
text_data <- pdf_text(pdf_path)
```


### Parsing Text into Paragraphs

```{r}
paragraph_text_tbl <- tibble(
    # Page Text
    page_text = text_data
) %>%
    rowid_to_column(var = "page_num") %>%
    
    # Paragraph Text
    mutate(paragraph_text = str_split(page_text, pattern = "\\.\n")) %>%
    select(-page_text) %>%
    unnest(paragraph_text) %>%
    rowid_to_column(var = "paragraph_num") %>%
    select(page_num, paragraph_num, paragraph_text)
    
paragraph_text_tbl
```

### Compare Text to Paragraphs

```{r}
image_read_pdf("pdf/Halliburton Announces First Quarter 2020 Results.pdf", pages = 6) %>%
    image_scale("600")
```



```{r}
paragraph_text_tbl %>%
    filter(page_num == 6)
```

## Prepare Text for Sentiment Analysis

### Paragraphs to Test

```{r}
paragraph_1 <- paragraph_text_tbl %>%
    slice(1) %>%
    pull(paragraph_text)

paragraph_2 <- paragraph_text_tbl %>%
    slice(2) %>%
    pull(paragraph_text)
```

```{r}
paragraph_1
```

```{r}
paragraph_2
```

### Corpus for Sentiment Model

Source: https://www.researchgate.net/publication/251231364_FinancialPhraseBank-v10

```{r, paged.print = FALSE}
financial_corpus_tbl <- read_delim("data/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt", 
                                   delim = "@", 
                                   col_names = FALSE) %>%
    set_names(c("paragraph_text", "sentiment")) %>%
    select(sentiment, paragraph_text) %>%
    mutate(sentiment = case_when(
        sentiment == "neutral"  ~ 0,
        sentiment == "positive" ~ 1,
        sentiment == "negative" ~ -1
    ))

financial_corpus_tbl
```

### Fix Issues

```{r}
# Fails when trying to convert to Pandas Data Frame
financial_corpus_tbl %>% slice(7)
```

```{r}
financial_corpus_clean_tbl <- financial_corpus_tbl %>%
    filter(!paragraph_text %>% str_detect("\\+")) %>%
    mutate(paragraph_text = str_remove_all(paragraph_text, "\\`")) 

financial_corpus_clean_tbl
```


# Python (TEXT SENTIMENT)

## Imports

```{python}
# Spacy & Text
import spacy
from spacy.lang.en import English
from spacy.lang.en.stop_words import STOP_WORDS
import string

# Data Manipulation
import numpy as np
import pandas as pd

# Scikit Learn
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.base import TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn import metrics
```

## Getting Paragraphs from R

```{python}
p1 = r.paragraph_1
p1
```

```{python}
p2 = r.paragraph_2
p2
```

```{python}
df_financial = r.financial_corpus_clean_tbl
df_financial
```


## Spacy Quick Tutorial


### Word Tokenization 

```{python}
# Load English tokenizer, tagger, parser, NER and word vectors
nlp = English()

#  "nlp" Object is used to create documents with linguistic annotations.
doc1 = nlp(p1)

# Create list of word tokens
token_list = []
for token in doc1:
    token_list.append(token.text)

print(token_list)
```

### Sentence Tokenization

```{python}
# Load English tokenizer, tagger, parser, NER and word vectors
nlp = English()

# Create the pipeline 'sentencizer' component
sbd = nlp.create_pipe('sentencizer')

# Add the component to the pipeline
nlp.add_pipe(sbd)

#  "nlp" Object is used to create documents with linguistic annotations.
doc = nlp(p1)

# create list of sentence tokens
sents_list = []
for sent in doc.sents:
    sents_list.append(sent.text)

counter = 1
for sent in sents_list:
    print(str(counter) + '. ' + sent)
    counter += 1
```

### Removing Stop Words

```{python}
spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS

print('Number of stop words: %d' % len(spacy_stopwords))
print('First ten stop words: %s' % list(spacy_stopwords)[:10])
```

```{python}
doc = nlp(p1)

filtered_sent=[]
for word in doc:
    if word.is_stop==False:
        filtered_sent.append(word)
  
print("Filtered Sentence:\n", filtered_sent)
```

### Word Stemming (Lemmatization)

```{python}
nlp = English()

# Implementing lemmatization
lem_test = nlp("Apples and oranges are similar. Boots and hippos aren't.")

# finding lemma for each word
for word in lem_test:
    print(word.text, word.lemma_)
```

## Sentiment Model (Classification)

### Tokenizer Setup

```{python}
# Create our list of punctuation marks
punctuations = string.punctuation

# Create our list of stopwords
stop_words = spacy.lang.en.stop_words.STOP_WORDS

# Load English tokenizer, tagger, parser, NER and word vectors
parser = English()
```

### Make Spacy Tokenizer

```{python}
# Creating our tokenizer function
def spacy_tokenizer(sentence):
    
    # Creating our token object, which is used to create documents with linguistic annotations.
    mytokens = parser(sentence)

    # Lemmatizing each token and converting each token into lowercase
    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != "-PRON-" else word.lower_ for word in mytokens ]

    # Removing stop words
    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]

    # return preprocessed list of tokens
    return mytokens
```

```{python}
sent = "Hey, this is a complex series of sentences. One with lots of WORDS in each sentence."
spacy_tokenizer(sent)
```

### Bag of Words Vectorizer

```{python}
bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))
```

```{python}
transformed = bow_vector.fit_transform(spacy_tokenizer(sent))
transformed
```

```{python}
transformed.toarray()
```

```{python}
bow_vector.get_feature_names()
```


### Train / Test

```{python}
X = df_financial['paragraph_text'] 
y = df_financial['sentiment'] 

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)
```

```{python}
X_train
```

```{python}
y_train
```

### Classification Model - Logistic Regression

```{python}
# Logistic Regression Classifier & Tokenization Setup
bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))
classifier = LogisticRegression()

# Create pipeline using Bag of Words
pipe = Pipeline([('vectorizer', bow_vector),
                 ('classifier', classifier)])

# model generation
pipe.fit(X_train, y_train)
```

```{python}
predicted = pipe.predict(X_test)

# Model Accuracy
print("Logistic Regression Accuracy:", metrics.accuracy_score(y_test, predicted))
```

```{python}
X_test
```


```{python}
predicted[:10]
```


```{python}
X_test[:10]
```

## Predict Halliburton Paragraphs

```{python}
pd.Series([p1,p2])
```


```{python}
pipe.predict(pd.Series([p1,p2]))
```

```{python}
df = r.paragraph_text_tbl
df
```

```{python}
halliburton_predictions = pipe.predict(df['paragraph_text'])
halliburton_predictions
```



## Sentiment Model (Regression)

### Regression Model - Linear Regression

```{python}
# Logistic Regression Classifier
classifier = LinearRegression()

# Create pipeline using Bag of Words
pipe_2 = Pipeline([('vectorizer', bow_vector),
                 ('classifier', classifier)])

# model generation
pipe_2.fit(X_train, y_train)
```

```{python}
predicted_2 = pipe_2.predict(X_test)

# Model Accuracy
print("Linear Regression MAE:", metrics.mean_absolute_error(y_test, predicted_2))
```

```{python}
halliburton_predictions_2 = pipe_2.predict(df['paragraph_text'])
halliburton_predictions_2
```

# Visualizations (Back to R)

```{r}
sentiment_predictions_tbl <- paragraph_text_tbl %>%
    mutate(
        sentiment_classification = py$halliburton_predictions,
        sentiment_regression     = py$halliburton_predictions_2
    )

sentiment_predictions_tbl
```



## Which Paragraphs are most Negative?

```{r}
data_prepared_tbl <- sentiment_predictions_tbl %>%
    mutate(label = str_glue("Page: {page_num}
                            Paragraph: {paragraph_num}
                            Sentiment: {round(sentiment_regression)}
                            ---
                            {paragraph_text}"))
```

```{r}
g <- data_prepared_tbl %>%
    mutate(sentiment_classification = case_when(
        sentiment_classification == 0  ~ "neutral",
        sentiment_classification == 1  ~ "positive",
        sentiment_classification == -1 ~ "negative"
    ) %>% factor(levels = c("negative", "neutral", "positive"))) %>%
    ggplot(aes(sentiment_classification, sentiment_regression, color = sentiment_regression)) +
    geom_point(aes(text = label, 
                   size = abs(sentiment_regression))) +
    scale_color_viridis_c() +
    theme_tq() +
    coord_flip()

ggplotly(g, tooltip = "text")
```

## Which Pages are Most Negative?

```{r}
g <- data_prepared_tbl %>%
    mutate(page_factor = page_num %>% as_factor() %>% fct_reorder(sentiment_regression)) %>%
    ggplot(aes(page_factor, sentiment_regression, color = sentiment_regression)) +
    geom_point(aes(text = label, 
                   size = abs(sentiment_regression))) +
    scale_color_viridis_c() +
    theme_tq() +
    coord_flip() +
    labs(
        title = "Sentiment By Page Number",
        x = "Page Number", y = "Sentiment Score"
    )

ggplotly(g, tooltip = "text")
```


# Save Python Models

```{python}
import joblib
```

```{python, eval=F}
joblib.dump(pipe, "models/pipe_logistic_regression.sav")
```

```{python, eval=F}
joblib.dump(pipe_2, "models/pipe_linear_regression.sav")
```

```{python}
joblib.load("models/pipe_logistic_regression.sav")
```

